{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue, size=\"6\">\n",
    "    <center>\n",
    "Part1-Module1 <br><br><br>\n",
    "Chapter 1. Introduction to Natural Language Processing\n",
    "</center>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue, size=\"6\">\n",
    "    <center>\n",
    "<br><br><br><br>GOOD NLP BOOK <br><br><br><br> http://www.ling.helsinki.fi/kit/2009s/clt231/NLTK/book/\n",
    "        </center>\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\python.exe \n",
      "\n",
      "['', 'D:\\\\ProgramData\\\\Anaconda3\\\\python36.zip', 'D:\\\\ProgramData\\\\Anaconda3\\\\DLLs', 'D:\\\\ProgramData\\\\Anaconda3\\\\lib', 'D:\\\\ProgramData\\\\Anaconda3', 'D:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages', 'D:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32', 'D:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'D:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'D:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\amits\\\\.ipython'] \n",
      "\n",
      "['C:\\\\Users\\\\amits\\\\AppData\\\\Roaming\\\\jupyter\\\\kernels', 'D:\\\\ProgramData\\\\Anaconda3\\\\share\\\\jupyter\\\\kernels', 'C:\\\\ProgramData\\\\jupyter\\\\kernels']\n",
      "<function jupyter_path at 0x000002177327F158>\n",
      "<function jupyter_config_dir at 0x0000021773276F28>\n",
      "<function jupyter_data_dir at 0x000002177327F048>\n",
      "<function jupyter_runtime_dir at 0x000002177327F0D0>\n",
      "<function jupyter_config_path at 0x000002177327F1E0>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable, \"\\n\")\n",
    "print(sys.path, \"\\n\")\n",
    "\n",
    "from jupyter_core.paths import jupyter_path\n",
    "print(jupyter_path('kernels'))\n",
    "print(jupyter_path)\n",
    "\n",
    "from jupyter_core.paths import jupyter_config_dir\n",
    "print(jupyter_config_dir)\n",
    "\n",
    "from jupyter_core.paths import jupyter_data_dir\n",
    "print(jupyter_data_dir)\n",
    "\n",
    "from jupyter_core.paths import jupyter_runtime_dir\n",
    "print(jupyter_runtime_dir)\n",
    "\n",
    "from jupyter_core.paths import jupyter_config_path\n",
    "print(jupyter_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4\n"
     ]
    }
   ],
   "source": [
    "# python -m pip install nltk -> to install nltk\n",
    "# python -m pip install -U nltk -> to upgrage nltk\n",
    "\n",
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue><center><strong>\n",
    "__Diving into NLTK__\n",
    "</strong></center></font>\n",
    "<font color=blue> Tokenization </font>may be defined as the process of splitting the text into smaller parts called tokens, and is considered a crucial step in NLP.\n",
    "\n",
    "<font color=blue>WordNet </font> is a dictionary designed for programmatic access by natural language processing systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://lxml.de/api/lxml.html.clean.Cleaner-class.html\n",
    "# http://lxml.de/lxmlhtml.html#cleaning-up-html.\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from lxml.html.clean import Cleaner\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "url = \"http://python.org/\"\n",
    "webpage = requests.get(url)\n",
    "soup_of_webpage = BeautifulSoup(webpage.text, \"lxml\")\n",
    "# print(soup_of_webpage.find_all('a'))\n",
    "# print(soup_of_webpage.get_text())\n",
    "cleaner = Cleaner(page_structure=True, javascript=True, remove_tags=['p'], remove_unknown_tags=True, links=True)\n",
    "cleaned = cleaner.clean_html(soup_of_webpage.get_text())\n",
    "english_stops = set(stopwords.words('english'))\n",
    "tokens = [tok for tok in cleaned.split() if len(tok.lower()) > 1 and (tok.lower() not in english_stops)]\n",
    "Freq_dist_dict = nltk.FreqDist(tokens)\n",
    "with open(\"frequency_count.csv\", 'w') as csv_file:\n",
    "\tfor key, value in Freq_dist_dict.items():\n",
    "\t\tprint(\"{0},{1}\".format(key, value), file=csv_file)\n",
    "\n",
    "Freq_dist_dict.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue, size=6> <center>\n",
    "Part1-Module1 <br><br><br>\n",
    "Chapter 2. Text Wrangling and Cleansing\n",
    "</center>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue><center><strong>\n",
    "        Extracting data from json file\n",
    "</strong></center></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key = array, value =[1, 2, 3, 4]\n",
      "key = boolean, value =True\n",
      "key = object, value ={'a': 'b'}\n",
      "key = string, value =Hello World\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create Dictionary\n",
    "my_dict = {'array': [1, 2, 3, 4], 'boolean': 'True', 'object': {'a': 'b'}, 'string': 'Hello World'}\n",
    "\n",
    "# Convert Dictionary into Json format using json.dumps\n",
    "my_json_var_to_dump_into_file = json.dumps(my_dict)\n",
    "\n",
    "# Write converted data into .json file\n",
    "with open(\"example.json\", 'w') as json_file:\n",
    "\tprint(my_json_var_to_dump_into_file, file=json_file)\n",
    "\n",
    "# Read the .json file\n",
    "with open(\"example.json\", 'r') as json_file:\n",
    "\t# convert json formatted data into dictionary format json.load()\n",
    "\tdata = json.load(json_file)\n",
    "\n",
    "# Iterate and print dictionary\n",
    "for key, value in data.items():\n",
    "\tprint(\"key = {}, value ={}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>\n",
    "__Tokenization__\n",
    "</font>\n",
    "<font color=blue> Tokenization </font>may be defined as the process of splitting the text into smaller parts called tokens, and is considered a crucial step in NLP.\n",
    "<font color=blue>WordNet </font> is a dictionary designed for programmatic access by natural language processing systems.\n",
    "<br><font color=green>__Tokenizing text into sentences__:</font> split the paragraph into sentences.\n",
    "A typical sentence splitter can be something as simple as\n",
    "splitting the string on (.), to something as complex as a predictive classifier to identify sentence boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "string = \"Welcome readers. I hope you find it interesting. Please do reply.\"\n",
    "print(sent_tokenize(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__sent_tokenize__, internally uses a sentence boundary detection algorithm that comes pre-built into\n",
    "NLTK. If your application requires a custom sentence splitter, there are ways that we can train a\n",
    "sentence splitter of our own.\n",
    "\n",
    "The __sent_tokenize__ function uses an instance of __PunktSentenceTokenizer__ from the __nltk.tokenize.punkt__ module. \n",
    "This instance has already been trained and works well for many European languages. \n",
    "So it knows what punctuation and characters mark the end of a sentence and the beginning of a new sentence.\n",
    "\n",
    "\n",
    "To tokenize a large number of sentences, we can load <font color=blue>PunktSentenceTokenizer </font> and use the <font color=blue>tokenize() </font> function to perform tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "string = \"Welcome readers. I hope you find it interesting. Please do reply.\"\n",
    "print(tokenizer.tokenize(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>__Tokenization of text in other languages__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assis en terrasse dans le quartier touristique du Vieux Lyon, James sirote une limonade bien fraîche.Avec la chaleur estivale, cette journée d’exploration se révèle très fatigante !Ce matin, James s’est levé tôt, a loué un vélo et a parcouru les rues de Lyon.',\n",
       " 'Il a longé les quais de la Saône, traversé la presqu’île, puis rejoint les quais du Rhône.',\n",
       " 'En chemin, il a pu admirer la magnifique fontaine Bartholdi sur la grande Place des Terreaux.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "french_tokenizer=nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "french_string = \"Assis en terrasse dans le quartier touristique du Vieux Lyon, James sirote une limonade bien fraîche.\" \\\n",
    "\t\t\t\t\"Avec la chaleur estivale, cette journée d’exploration se révèle très fatigante !Ce matin, \" \\\n",
    "\t\t\t\t\"James s’est levé tôt, a loué un vélo et a parcouru les rues de Lyon. \" \\\n",
    "\t\t\t\t\"Il a longé les quais de la Saône, traversé la presqu’île, puis rejoint les quais du Rhône. \" \\\n",
    "\t\t\t\t\"En chemin, il a pu admirer la magnifique fontaine Bartholdi sur la grande Place des Terreaux.\"\n",
    "french_tokenizer.tokenize(french_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>__Tokenization of sentences into words__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <font color=blue> __word_tokenize__ </font> function uses an instance of NLTK known as <font color=blue>__TreebankWordTokenizer__ </font> to perform word tokenization.<br>It works by separating words using spaces and punctuation\n",
    "<font color=red>\n",
    "__Other Tokenizers are:<br>\n",
    "TreebankWordTokenizer()<br>\n",
    "WordPunctTokenizer()<br>\n",
    "WhitespaceTokenizer()<br>\n",
    "LineTokenizer()<br>\n",
    "RegexpTokenizer()<br>__\n",
    "<br><br>\n",
    "__Difference is how they handel punctuation if word if \"can't\" then<br>\n",
    "word_tokenize(\"can't\") -> ['ca', \"n't\"]<br>\n",
    "PunktWordTokenizer(\"can't\") -> ['Can', \"'t\"]<br>\n",
    "WordPunctTokenizer(\"can't\") -> ['Can', \"'\", 't']__\n",
    "<\\font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'a', 'nice', 'day', '.', 'do', \"n't\", 'find', 'this', 'book', 'gr8', 'and', 'interesting']\n",
      "['Have', 'a', 'nice', 'day.', 'do', \"n't\", 'find', 'this', 'book', 'gr8', 'and', 'interesting']\n",
      "['Have', 'a', 'nice', 'day', '.', 'don', \"'\", 't', 'find', 'this', 'book', 'gr8', 'and', 'interesting']\n",
      "['Have', 'a', 'nice', 'day.', \"don't\", 'find', 'this', 'book', 'gr8', 'and', 'interesting']\n",
      "['Have', 'a', 'nice', 'day', 'don', 't', 'find', 'this', 'book', 'gr8', 'and', 'interesting']\n",
      "['8']\n",
      "['Have a nice day.', \"don't find this book gr8 and interesting\"]\n",
      "['Have a nice day. ', \"don't find this book gr8 and interesting\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import \\\n",
    "\t(TreebankWordTokenizer, WordPunctTokenizer, word_tokenize, regexp_tokenize, blankline_tokenize, WhitespaceTokenizer, \\\n",
    "    LineTokenizer)\n",
    "\n",
    "# PunktWordTokenizer not able to import\n",
    "\n",
    "tokenizer1 = TreebankWordTokenizer()\n",
    "tokenizer2 = WordPunctTokenizer()\n",
    "tokenizer3 = WhitespaceTokenizer()\n",
    "tokenizer4 = LineTokenizer()\n",
    "\n",
    "pattern1 = r'\\w+'  # which means we need all the words and digits from the string,\n",
    "                  # and other symbols can be used as a splitter\n",
    "pattern2 ='\\d+'\n",
    "string = \"Have a nice day. \\n\\n\\n\\n\\n\\n\\ndon't find this book gr8 and interesting\"\n",
    "\n",
    "print(word_tokenize(string))             #  'day', '.',   'do', \"n't\",\n",
    "print(tokenizer1.tokenize(string))       #  'day.',       'do', \"n't\",\n",
    "print(tokenizer2.tokenize(string))       #  'day', '.',   'don', \"'\" , 't',\n",
    "print(tokenizer3.tokenize(string))       # 'day.',        \"don't\", 'find', 'this',  \n",
    "print(regexp_tokenize(string, pattern1)) # 'day',         'don', 't', 'find',\n",
    "print(regexp_tokenize(string, pattern2)) # ['8']\n",
    "print(blankline_tokenize(string))        # ['Have a nice day.', \"don't find this book gr8 and interesting\"]   \n",
    "print(tokenizer4.tokenize(string))       # ['Have a nice day. ', \"don't find this book gr8 and interesting\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> \n",
    "    nltk.tokenize.util \n",
    "</font> module works by returning the sequence of tuples that are offsets of the tokens in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object RegexpTokenizer.span_tokenize at 0x000002177DA88EB8>\n",
      "[(0, 3), (4, 11), (12, 17), (18, 19), (20, 22), (23, 28), (29, 31), (32, 35), (36, 38), (39, 40), (41, 52), (53, 60)]\n",
      "<generator object spans_to_relative at 0x000002177DF58410>\n",
      "[(0, 3), (1, 7), (1, 5), (1, 1), (1, 2), (1, 5), (1, 2), (1, 3), (1, 2), (1, 1), (1, 11), (1, 7)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize.util import spans_to_relative\n",
    "string=\"She secured 90.56 % in class X.\\nShe is a meritorious student\\n\"\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "print(tokenizer.span_tokenize(string))\n",
    "print(list(tokenizer.span_tokenize(string)))\n",
    "\n",
    "print(spans_to_relative(tokenizer.span_tokenize(string)))\n",
    "print(list(spans_to_relative(tokenizer.span_tokenize(string))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<font color=blue>\n",
    "__Normalization__\n",
    "</font>\n",
    "In order to carry out processing on natural language text, we need to perform normalization that mainly involves eliminating punctuation, converting the entire text into lowercase or uppercase, converting numbers into words, expanding\n",
    "abbreviations, canonicalization of text, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " List with punctuation : [['It', 'is', 'a', 'pleasant', 'evening', '.'], ['Guests', ',', 'who', 'came', 'from', '``', 'US', \"''\", 'arrived', 'at', 'the', 'venue', '?'], ['Food', 'was', 'not', 'tasty', '.']]\n",
      "\n",
      "\n",
      " List without punctuation : [['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', '``', 'US', \"''\", 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'not', 'tasty']]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string_of_english_punctuation = string.punctuation\n",
    "text_list = ['It is a pleasant evening.','Guests, who came from \"US\" arrived at the venue?','Food was not tasty.']\n",
    "tokenized_text_list = [ word_tokenize(sentence) for sentence in text_list ]\n",
    "\n",
    "print(\"\\n\\n List with punctuation :\", tokenized_text_list)\n",
    "\n",
    "blank_main_list = []\n",
    "for item in tokenized_text_list:\n",
    "    secondary_blank_list = []\n",
    "    for word in item:\n",
    "        if word not in string_of_english_punctuation:\n",
    "            secondary_blank_list.append(word)\n",
    "    blank_main_list.append(secondary_blank_list)\n",
    "\n",
    "print(\"\\n\\n List without punctuation :\", blank_main_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><font color=blue>\n",
    "__Stemming__\n",
    "</font><br>\n",
    "process of cutting down the branches of a tree to its stem. So effectively, with the use of some basic rules, \n",
    "any token can be cut down to its stem.<br>\n",
    "In some applications, as it does not make sense to differentiate between eat and eaten, we typically use stemming to club both\n",
    "grammatical variances to the root of the word. While stemming is used most of the time for its\n",
    "simplicity, there are cases of complex language or complex NLP tasks where it's necessary to use\n",
    "lemmatization instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook\n",
      "cookery\n",
      "cook\n",
      "cookeri\n",
      "cook\n",
      "cookery\n",
      "___________________\n",
      "met\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nest'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer # import Porter stemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "pattern = r'ing'\n",
    "\n",
    "lst = LancasterStemmer() # create obj of LancasterStemmer\n",
    "pst = PorterStemmer() # create obj of the PorterStemmer\n",
    "rst = RegexpStemmer(pattern)\n",
    "\n",
    "print(lst.stem(\"cooking\"))  # cook\n",
    "print(lst.stem(\"cookery\"))  # cookery\n",
    "print(pst.stem(\"cooking\"))  # cook\n",
    "print(pst.stem(\"cookery\"))  # cookeri - he stem of cookery is cookeri. This is a feature, not a bug.\n",
    "print(rst.stem(\"cooking\"))  # cook\n",
    "print(rst.stem(\"cookery\"))  # cookery\n",
    "print(\"___________________\")\n",
    "\n",
    "lst = LancasterStemmer(strip_prefix_flag=True)\n",
    "print(lst.stem('kilometer'))  # met # Test Prefix\n",
    "\n",
    "lst_custom = LancasterStemmer(rule_tuple=(\"ssen4>\", \"s1t.\"))\n",
    "lst_custom.stem(\"ness\") # Change s to t # nest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>__The SnowballStemmer class__</font><br><br>\n",
    "The __SnowballStemmer class__ supports 13 non-English languages. It also provides two\n",
    "English stemmers: <br>the original porter algorithm as well as the new English stemming\n",
    "algorithm. <br><br> To use the SnowballStemmer class, create an instance with the name of the\n",
    "language you are using and then call the stem() method. Here is a list of all the supported\n",
    "languages and an example using the Spanish SnowballStemmer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "hol\n",
      "generous\n",
      "gener\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "print(SnowballStemmer.languages)\n",
    "spanish_stemmer = SnowballStemmer('spanish')\n",
    "print(spanish_stemmer.stem('hola'))\n",
    "\n",
    "# The 'english' stemmer is better than the original 'porter' stemmer.\n",
    "print(SnowballStemmer(\"english\").stem(\"generously\"))\n",
    "print(SnowballStemmer(\"porter\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://research.variancia.com/hindi_stemmer/\n",
    "<font color=green><br>__CISTEM Stemmer for German__<br></font>\n",
    "http://www.nltk.org/api/nltk.stem.html<br>\n",
    "But most users can live with Porter and Snowball stemmer for a large number of use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grenzpost\n",
      "('grenzpost', 'ens')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import cistem \n",
    "\n",
    "cst = cistem.Cistem(case_insensitive=False)\n",
    "german_string = \"Grenzpostens\"\n",
    "print(cst.stem(german_string))\n",
    "print(cst.segment(german_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><font color=blue>\n",
    "__Lemmatization__\n",
    "</font><br><br>\n",
    "Lemmatization is very similar to stemming, but is more akin to synonym replacement.\n",
    "A lemma is a root word, as opposed to the root stem. So unlike stemming, you are always\n",
    "left with a valid word that means the same thing. However, the word you end up with can\n",
    "be completely different. A few examples will explain this.<br><br>Lemmatize using WordNet’s built-in morphy function. Returns the input word unchanged if it cannot be found in WordNet.<br><br>As demonstrated below, cooking does not\n",
    "return a different lemma unless you specify that the POS is a verb. This is because the default\n",
    "POS is a noun, and as a noun, cooking is its own lemma. On the other hand, cookbooks\n",
    "is a noun with its singular form, cookbook, as its lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookbook\n",
      "dog\n",
      "church\n",
      "aardwolf\n",
      "abacus\n",
      "hardrock\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('cooking'))\n",
    "print(lemmatizer.lemmatize('cooking', pos='v'))\n",
    "print(lemmatizer.lemmatize('cookbooks'))\n",
    "print(lemmatizer.lemmatize('dogs'))\n",
    "print(lemmatizer.lemmatize('churches'))\n",
    "print(lemmatizer.lemmatize('aardwolves'))\n",
    "print(lemmatizer.lemmatize('abaci'))\n",
    "print(lemmatizer.lemmatize('hardrock'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fileinput\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for line in fileinput.input('1865-Lincoln.txt', inplace=True, backup='.bak'):\n",
    "    line = ' '.join(\n",
    "        [lemmatizer.lemmatize(w) for w in line.rstrip().split()]\n",
    "    )\n",
    "    # overwrites current `line` in file\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><font color=green>__differences between stemming and lemmatization__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believ\n",
      "belief\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(stemmer.stem('believes'))\n",
    "print(lemmatizer.lemmatize('believes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Instead of just chopping off the es like the PorterStemmer class, the\n",
    "WordNetLemmatizer class finds a valid root word. Where a stemmer only looks at the form\n",
    "of the word, the lemmatizer looks at the meaning of the word. By returning a lemma, you will\n",
    "always get a valid word.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><font color=green>__Combining stemming with lemmatization__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buse\n",
      "bus\n",
      "bu\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(stemmer.stem('buses'))\n",
    "print(lemmatizer.lemmatize('buses'))\n",
    "print(stemmer.stem('bus'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, stemming saves one character, lemmatization saves two characters, and\n",
    "stemming the lemma saves a total of three characters out of five characters. That is nearly\n",
    "a 60% compression rate! This level of word compression over many thousands of words,\n",
    "while unlikely to always produce such high gains, can still make a huge difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><font color=blue><font color=blue>\n",
    "__Replacing words matching regular expressions__\n",
    "</font></font><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot is a contradicton\n"
     ]
    }
   ],
   "source": [
    "from replacers import RegexpReplacer\n",
    "replacer = RegexpReplacer()\n",
    "print(replacer.replace(\"can't is a contradicton\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><font color=blue>\n",
    "__Stop word removal__\n",
    "</font></font><br>\n",
    "NLTK\n",
    "comes with a pre-built list of stop words for around 22 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'test']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english') # config the language name\n",
    "text = \"This is just a test\"\n",
    "cleanwordlist = [word for word in text.split() if word not in stoplist]\n",
    "\n",
    "print(cleanwordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br>\n",
    "<font color=blue>\n",
    "__Rare word removal__\n",
    "</font><br>\n",
    "Remove the words which occur very less number of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><font color=blue>\n",
    "    __Spell correction__\n",
    "</font></font><br>\n",
    "It is not a necessary to use a spellchecker for all NLP applications, but some use cases require you to use\n",
    "a basic spellcheck. We can create a very basic spellchecker by just using a dictionary lookup. There are\n",
    "some enhanced string algorithms that have been developed for fuzzy string matching. One of the most\n",
    "commonly used is edit-distance. NLTK also provides you with a variety of metrics module that\n",
    "has edit_distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "edit_distance(\"rain\",\"shine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spelling\n",
      "corrected\n",
      "bites\n",
      "later\n",
      "vacuum\n",
      "wrong\n"
     ]
    }
   ],
   "source": [
    "# http://norvig.com/spell-correct.html\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# words function read the passed large string and convert it into list of lowercase words.List is passed to Counter.\n",
    "def words(text):\n",
    "\treturn re.findall(r'\\w+', text.lower())\n",
    "\n",
    "\n",
    "# read the file big.txt in one large string and that string is passwd to words function.\n",
    "# words function read the passed large string and convert it into list of lowercase words.List is passed to Counter.\n",
    "# Counter converts list into dictionary like object called as collections.Counter with the frequency of each word and\n",
    "# store it in variable WORDS collections.Counter object.\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "\n",
    "# how to calculate probability (P) of word? \n",
    "# P(word) = (Number of favourable outcome) / (Total number of outcomes)\n",
    "# remember dice question probability of coming number 2 , P(2) = 1/6\n",
    "# Denominator : i.e total number of outcome = sum of total words present in WORDS dictionary.\n",
    "# Numenator : number of time word occur in WORDS dictionary.\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "\t\"\"\"Probability of `word`.\"\"\"\n",
    "\treturn WORDS[word] / N\n",
    "\n",
    "\n",
    "def correction(word):\n",
    "\t\"\"\"Most probable spelling correction for word.\"\"\"\n",
    "\treturn max(candidates(word), key=P)\n",
    "\n",
    "\n",
    "def candidates(word):\n",
    "\t\"\"\"Generate possible spelling corrections for word.\"\"\"\n",
    "\treturn known([word]) or known(edits1(word)) or known(edits2(word)) or [word]\n",
    "\n",
    "\n",
    "def known(words):\n",
    "\t\"\"\"The subset of `words` that appear in the dictionary of WORDS.\"\"\"\n",
    "\treturn set(w for w in words if w in WORDS)\n",
    "\n",
    "\n",
    "def edits1(word):\n",
    "\t\"\"\"All edits that are one edit away from `word`.\"\"\"\n",
    "\t\"\"\"edits1 function will return set{}\"\"\"\n",
    "\tletters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\tsplits = [(word[:i], word[i:]) for i in range(len(word) + 1)]  \n",
    "    # splits will provide result as a list of tuples:\n",
    "    # if we pass a to edits1 then splits = [('', 'a'), ('a', '')]\n",
    "    # if we pass ab to edits1 then splits = [('', 'ab'), ('a', 'b'), ('ab', '')]\n",
    "    # if we pass abc to edits1 then splits = [('', 'abc'), ('a', 'bc'), ('ab', 'c'), ('abc', '')]\n",
    "    # if we pass abcd to edits then splits = [('', 'abcd'), ('a', 'bcd'), ('ab', 'cd'), ('abc', 'd'), ('abcd', '')]\n",
    "    # if we pass abcde to edits then splits = [('', 'abcde'), ('a', 'bcde'), ('ab', 'cde'), ('abc', 'de'), ('abcd', 'e'), ('abcde', '')]\n",
    "\tdeletes = [L + R[1:] for L, R in splits if R]\n",
    "    # deletes will provide result as a list:\n",
    "    # if we pass a to edits1 then deletes = ['']\n",
    "    # if we pass ab to edits1 then deletes = ['b', 'a']\n",
    "    # if we pass abc to edits1 then deletes = ['bc', 'ac', 'ab']\n",
    "    # if we pass abcd to edits1 then deletes = ['bcd', 'acd', 'abd', 'abc']\n",
    "    # if we pass abcde to edits1 then deletes = ['bcde', 'acde', 'abde', 'abce', 'abcd']\n",
    "\ttransposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    # transposes will provide result as a list:\n",
    "    # if we pass a to edits1 then transposes = []\n",
    "    # if we pass ab to edits1 then transposes = ['ba']\n",
    "    # if we pass abc to edits1 then transposes = ['bac', 'acb']\n",
    "    # if we pass abcd to edits1 then transposes = ['bacd', 'acbd', 'abdc']\n",
    "    # if we pass abcde to edits1 then transposes = ['bacde', 'acbde', 'abdce', 'abced']\n",
    "\treplaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    # if we pass a to edits1 then replaces = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', \n",
    "    # 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    \n",
    "    # if we pass ab to edits1 then replaces = ['ab', 'bb', 'cb', 'db', 'eb', 'fb', 'gb', 'hb', 'ib', 'jb', 'kb', 'lb', \n",
    "    # 'mb', 'nb', 'ob', 'pb', 'qb', 'rb', 'sb', 'tb', 'ub', 'vb', 'wb', 'xb', 'yb', 'zb', 'aa', 'ab', 'ac', 'ad', 'ae', \n",
    "    # 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', \n",
    "    # 'ay', 'az']\n",
    "    \n",
    "    # if we pass abc to edits1 then replaces = ['abc', 'bbc', 'cbc', 'dbc', 'ebc', 'fbc', 'gbc', 'hbc', 'ibc', 'jbc', \n",
    "    # 'kbc', 'lbc', 'mbc', 'nbc', 'obc', 'pbc', 'qbc', 'rbc', 'sbc', 'tbc', 'ubc', 'vbc', 'wbc', 'xbc', 'ybc', 'zbc', \n",
    "    # 'aac', 'abc', 'acc', 'adc', 'aec', 'afc', 'agc', 'ahc', 'aic', 'ajc', 'akc', 'alc', 'amc', 'anc', 'aoc', 'apc', \n",
    "    # 'aqc', 'arc', 'asc', 'atc', 'auc', 'avc', 'awc', 'axc', 'ayc', 'azc', 'aba', 'abb', 'abc', 'abd', 'abe', 'abf', \n",
    "    # 'abg', 'abh', 'abi', 'abj', 'abk', 'abl', 'abm', 'abn', 'abo', 'abp', 'abq', 'abr', 'abs', 'abt', 'abu', 'abv', \n",
    "    # 'abw', 'abx', 'aby', 'abz']\n",
    "\tinserts = [L + c + R for L, R in splits for c in letters]\n",
    "    # if we pass a to edits1 then inserts = ['aa', 'ba', 'ca', 'da', 'ea', 'fa', 'ga', 'ha', 'ia', 'ja', 'ka', 'la', \n",
    "    # 'ma', 'na', 'oa', 'pa', 'qa', 'ra', 'sa', 'ta', 'ua', 'va', 'wa', 'xa', 'ya', 'za', 'aa', 'ab', 'ac', 'ad', 'ae', \n",
    "    # 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', \n",
    "    # 'ay', 'az']\n",
    "    \n",
    "    # if we pass ab to edits1 then inserts = ['aab', 'bab', 'cab', 'dab', 'eab', 'fab', 'gab', 'hab', 'iab', 'jab', \n",
    "    # 'kab', 'lab', 'mab', 'nab', 'oab', 'pab', 'qab', 'rab', 'sab', 'tab', 'uab', 'vab', 'wab', 'xab', 'yab', 'zab', \n",
    "    # 'aab', 'abb', 'acb', 'adb', 'aeb', 'afb', 'agb', 'ahb', 'aib', 'ajb', 'akb', 'alb', 'amb', 'anb', 'aob', 'apb', \n",
    "    # 'aqb', 'arb', 'asb', 'atb', 'aub', 'avb', 'awb', 'axb', 'ayb', 'azb', 'aba', 'abb', 'abc', 'abd', 'abe', 'abf', \n",
    "    # 'abg', 'abh', 'abi', 'abj', 'abk', 'abl', 'abm', 'abn', 'abo', 'abp', 'abq', 'abr', 'abs', 'abt', 'abu', 'abv', \n",
    "    # 'abw', 'abx', 'aby', 'abz']\n",
    "    \n",
    "    # if we pass abc to edits1 then inserts = ['aabc', 'babc', 'cabc', 'dabc', 'eabc', 'fabc', 'gabc', 'habc', 'iabc', \n",
    "    # 'jabc', 'kabc', 'labc', 'mabc', 'nabc', 'oabc', 'pabc', 'qabc', 'rabc', 'sabc', 'tabc', 'uabc', 'vabc', 'wabc', \n",
    "    # 'xabc', 'yabc', 'zabc', 'aabc', 'abbc', 'acbc', 'adbc', 'aebc', 'afbc', 'agbc', 'ahbc', 'aibc', 'ajbc', 'akbc', \n",
    "    # 'albc', 'ambc', 'anbc', 'aobc', 'apbc', 'aqbc', 'arbc', 'asbc', 'atbc', 'aubc', 'avbc', 'awbc', 'axbc', 'aybc', \n",
    "    # 'azbc', 'abac', 'abbc', 'abcc', 'abdc', 'abec', 'abfc', 'abgc', 'abhc', 'abic', 'abjc', 'abkc', 'ablc', 'abmc',\n",
    "    # 'abnc', 'aboc', 'abpc', 'abqc', 'abrc', 'absc', 'abtc', 'abuc', 'abvc', 'abwc', 'abxc', 'abyc', 'abzc', 'abca', \n",
    "    # 'abcb', 'abcc', 'abcd', 'abce', 'abcf', 'abcg', 'abch', 'abci', 'abcj', 'abck', 'abcl', 'abcm', 'abcn', 'abco', \n",
    "    # 'abcp', 'abcq', 'abcr', 'abcs', 'abct', 'abcu', 'abcv', 'abcw', 'abcx', 'abcy', 'abcz']\n",
    "\treturn set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "def edits2(word):\n",
    "\t\"\"\"All edits that are two edits away from `word`.\"\"\"\n",
    "\treturn (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "print(correction('speling'))\n",
    "print(correction('korrectud'))\n",
    "print(correction(\"kites\"))\n",
    "print(correction(\"lates\"))\n",
    "print(correction(\"vacucme\"))\n",
    "print(correction(\"wrng\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Can we perform other NLP operations after stop word removal?__<br>\n",
    "No; never. All the typical NLP applications like POS tagging, chunking, and so on will need\n",
    "context to generate the tags for the given text. Once we remove the stop word, we lose the\n",
    "context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><font color=blue>\n",
    "    __Looking up Synsets for a word in WordNet__\n",
    "</font></font><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WordNet__ is a lexical database for the English language. In other words, it's a dictionary\n",
    "designed specifically for natural language processing.\n",
    "<br><br><br>\n",
    "NLTK comes with a simple interface to look up words in WordNet. What you get is a list of\n",
    "__Synset__ instances (The list may be empty if the word is not found), which are groupings of synonymous words that express the same concept.Many words have only one Synset, but some have several.\n",
    "\n",
    "__Synsets__ are organized in a structure similar to that of an inheritance tree. \n",
    "More abstract terms are known as __hypernyms__ and more specific terms are __hyponyms.__\n",
    "\n",
    "This tree can be traced all the way up to a root hypernym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cookbook.n.01\n",
      "a book of recipes and cooking directions\n",
      "[]\n",
      "[Synset('reference_book.n.01')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "name ():      The name() method will give you a unique name for the Synset, which you can use to get the Synset directly\n",
    "definition(): The definition() method should be self-explanatory. \n",
    "examples():   Some Synsets also have an examples() method, which contains a list of phrases that use the word in context\n",
    "hypernym_paths(): The hypernym_paths() method returns a list of lists, where each list starts at the root hypernym and ends \n",
    "with the original Synset. Most of the time, you'll only get one nested list of Synsets.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets('cookbook')[0]                  \n",
    "print(syn.name())                                     # cookbook.n.01\n",
    "print(syn.definition())                               # a book of recipes and cooking directions\n",
    "print(syn.examples())                                 # []\n",
    "print(syn.hypernyms())                                # [Synset('reference_book.n.01')]\n",
    "# print(syn.hypernyms()[0].hyponyms())                  \n",
    "# \"\"\" [Synset('annual.n.02'), Synset('atlas.n.02'), Synset('cookbook.n.01'), Synset('directory.n.01'), \n",
    "#      Synset('encyclopedia.n.01'), Synset('handbook.n.01'), Synset('instruction_book.n.01'), Synset('source_book.n.01'), \n",
    "#      Synset('wordbook.n.01')]\n",
    "# \"\"\"\n",
    "# print(syn.root_hypernyms())                            # [Synset('entity.n.01')]\n",
    "\n",
    "# # can trace the entire path from entity down to cookbook using the hypernym_paths() method,\n",
    "# print(syn.hypernym_paths())\n",
    "# \"\"\"[[Synset('entity.n.01'), Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), \n",
    "#      Synset('artifact.n.01'), Synset('creation.n.02'), Synset('product.n.02'), Synset('work.n.02'), \n",
    "#      Synset('publication.n.01'), Synset('book.n.01'), Synset('reference_book.n.01'), Synset('cookbook.n.01')]]]\n",
    "# \"\"\"\n",
    "\n",
    "# print(\"_____________________________________________________\")\n",
    "\n",
    "\n",
    "# syn = wordnet.synsets('cooking')[0]\n",
    "# print(syn.name())                                        # cooking.n.01\n",
    "# print(syn.definition())\n",
    "# \"\"\"['cooking can be a great art', 'people are needed who have experience in cookery', \n",
    "#      'he left the preparation of meals to his wife']\n",
    "# \"\"\"\n",
    "# print(syn.examples())                                    # [Synset('change_of_state.n.01')]\n",
    "# print(syn.hypernyms())\n",
    "# print(syn.hypernyms()[0].hyponyms())\n",
    "# \"\"\"\n",
    "# [Synset('aeration.n.02'), Synset('arousal.n.01'), Synset('beautification.n.01'), Synset('beginning.n.05'), \n",
    "# Synset('change_of_color.n.01'), Synset('chew.n.02'), Synset('cooking.n.01'), Synset('decoration.n.03'), \n",
    "# Synset('defoliation.n.02'), Synset('degradation.n.01'), Synset('improvement.n.02'), Synset('infusion.n.04'), \n",
    "# Synset('meddling.n.01'), Synset('nullification.n.02'), Synset('passage.n.01'), Synset('reversal.n.01'), \n",
    "# Synset('seasoning.n.02'), Synset('soiling.n.01'), Synset('specialization.n.01'), Synset('spiritualization.n.01'), \n",
    "# Synset('termination.n.05'), Synset('transfer.n.03'), Synset('wetting.n.01'), Synset('worsening.n.02')]\n",
    "# \"\"\"\n",
    "# print(syn.root_hypernyms())                              # [Synset('entity.n.01')]\n",
    "# print(syn.hypernym_paths())\n",
    "# \"\"\"\n",
    "# [[Synset('entity.n.01'), Synset('abstraction.n.06'), Synset('psychological_feature.n.01'), Synset('event.n.01'), \n",
    "# Synset('act.n.02'), Synset('action.n.01'), Synset('change.n.03'), Synset('change_of_state.n.01'), Synset('cooking.n.01')]]\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue, size=6> <center>\n",
    "Part1-Module1 <br><br><br>\n",
    "Chapter Extra. Creating Custom Corpora\n",
    "</center>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A corpus is a collection of text documents, and corpora is the plural of corpus.<br> So a custom corpus is really just\n",
    "a bunch of text files in a directory, often alongside many other directories of text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK defines a list of data directories, or paths, in nltk.data.path. Our custom corpora\n",
    "must be within one of these paths so it can be found by NLTK. In order to avoid conflict with\n",
    "the official data package, we'll create a custom nltk_data directory in our home directory.\n",
    "The following is some Python code to create this directory and verify that it is in the list of\n",
    "known paths specified by nltk.data.path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue, size=6> <center>\n",
    "Part1-Module1 <br><br><br>\n",
    "Chapter 3. Part of Speech Tagging\n",
    "</center>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is Part of speech tagging?__<br>\n",
    "Schools commonly teach that there are 9 parts of speech in English:\n",
    "<br>noun, <br>verb, <br>article, <br>adjective, <br>preposition, <br>pronoun, <br>adverb, <br>conjunction and <br>interjection. <br>However, there are clearly many more categories and sub-categories\n",
    "\n",
    "<br>__Part-of-speech__ tagging is the process of converting a sentence, in the form of a list of words,\n",
    "into a list of tuples, where each tuple is of the form (word, tag). The tag is a part-of-speech\n",
    "tag, and signifies whether the word is a noun, adjective, verb, and so on.\n",
    "\n",
    "<br>All taggers in NLTK are in the __nltk.tag__ package and inherit from the __TaggerI__ base class.\n",
    "__TaggerI__ requires all subclasses to implement a __tag() method, which takes a list of words\n",
    "as input and returns a list of tagged words as output__. TaggerI also provides an __evaluate()__\n",
    "method for evaluating the accuracy of the tagger\n",
    "\n",
    "When we talk about POS, the most frequent POS notification used is Penn Treebank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Tag </td><td>Description                        </td></tr>\n",
       "<tr><td>NNP </td><td>Proper noun singular               </td></tr>\n",
       "<tr><td>NNPS</td><td>Proper noun plural                 </td></tr>\n",
       "<tr><td>PDT </td><td>Pre determiner                     </td></tr>\n",
       "<tr><td>POS </td><td>Possessive ending                  </td></tr>\n",
       "<tr><td>PRP </td><td>Personal pronoun                   </td></tr>\n",
       "<tr><td>PRP$</td><td>Possessive pronoun                 </td></tr>\n",
       "<tr><td>RB  </td><td>Adverb                             </td></tr>\n",
       "<tr><td>RBR </td><td>Adverb comparative                 </td></tr>\n",
       "<tr><td>RBS </td><td>Adverb superlative                 </td></tr>\n",
       "<tr><td>RP  </td><td>Particle                           </td></tr>\n",
       "<tr><td>SYM </td><td>Symbol (mathematical or scientific)</td></tr>\n",
       "<tr><td>TO  </td><td>to                                 </td></tr>\n",
       "<tr><td>UH  </td><td>Interjection                       </td></tr>\n",
       "<tr><td>VB  </td><td>Verb base form                     </td></tr>\n",
       "<tr><td>VBD </td><td>Verb past tense                    </td></tr>\n",
       "<tr><td>VBG </td><td>Verb gerund/present participle     </td></tr>\n",
       "<tr><td>VBN </td><td>Verb past                          </td></tr>\n",
       "<tr><td>WP  </td><td>Wh-pronoun                         </td></tr>\n",
       "<tr><td>WP$ </td><td>Possessive wh-pronoun              </td></tr>\n",
       "<tr><td>WRB </td><td>Wh-adverb                          </td></tr>\n",
       "<tr><td>#   </td><td>Pound sign                         </td></tr>\n",
       "<tr><td>$   </td><td>Dollar sign                        </td></tr>\n",
       "<tr><td>.   </td><td>Sentence-final punctuation         </td></tr>\n",
       "<tr><td>,   </td><td>Comma                              </td></tr>\n",
       "<tr><td>:   </td><td>Colon, semi-colon                  </td></tr>\n",
       "<tr><td>(   </td><td>Left bracket character             </td></tr>\n",
       "<tr><td>)   </td><td>Right bracket character            </td></tr>\n",
       "<tr><td>\"   </td><td>Straight double quote              </td></tr>\n",
       "<tr><td>'   </td><td>Left open single quote             </td></tr>\n",
       "<tr><td>\"   </td><td>Left open double quote             </td></tr>\n",
       "<tr><td>'   </td><td>Right close single quote           </td></tr>\n",
       "<tr><td>\"   </td><td>Right open double quote            </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "table = [['Tag', 'Description'],\n",
    "['NNP', 'Proper noun singular'],\n",
    "['NNPS', 'Proper noun plural'],\n",
    "['PDT', 'Pre determiner'],\n",
    "['POS', 'Possessive ending'],\n",
    "['PRP', 'Personal pronoun'],\n",
    "['PRP$', 'Possessive pronoun'],\n",
    "['RB', 'Adverb'],\n",
    "['RBR', 'Adverb comparative'],\n",
    "['RBS', 'Adverb superlative'],\n",
    "['RP', 'Particle'],\n",
    "['SYM', 'Symbol (mathematical or scientific)'],\n",
    "['TO', 'to'],\n",
    "['UH', 'Interjection'],\n",
    "['VB',  'Verb base form'],\n",
    "['VBD', 'Verb past tense'],\n",
    "['VBG', 'Verb gerund/present participle'],\n",
    "['VBN', 'Verb past'],\n",
    "['WP', 'Wh-pronoun'],\n",
    "['WP$', 'Possessive wh-pronoun'],\n",
    "['WRB', 'Wh-adverb'],\n",
    "['#', 'Pound sign'],\n",
    "['$', 'Dollar sign'],\n",
    "['.', 'Sentence-final punctuation'],\n",
    "[',', 'Comma'],\n",
    "[':', 'Colon, semi-colon'],\n",
    "['(', 'Left bracket character'],\n",
    "[')', 'Right bracket character'],\n",
    "['\"', 'Straight double quote'],\n",
    "[\"'\", 'Left open single quote'],\n",
    "['\"', 'Left open double quote'],\n",
    "[\"'\", 'Right close single quote'],\n",
    "['\"', 'Right open double quote']]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('was', 'VBD'), ('watching', 'VBG'), ('TV', 'NN')]\n",
      "[('Илья', 'S'), ('оторопел', 'V'), ('и', 'CONJ'), ('дважды', 'ADV'), ('перечитал', 'V'), ('бумажку', 'S'), ('.', 'NONLEX')]\n",
      "['TV']\n",
      "['was', 'watching']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag   # Currently only supoorts English and Russian.\n",
    "from nltk import word_tokenize\n",
    "\n",
    "string = \"I was watching TV\"\n",
    "print(pos_tag(word_tokenize(string)))\n",
    "print(pos_tag(word_tokenize(\"Илья оторопел и дважды перечитал бумажку.\"), lang='rus'))\n",
    "\n",
    "# How to fetch all NOUN from text? \n",
    "tagged = nltk.pos_tag(word_tokenize(string))\n",
    "allnoun = [word for word,pos in tagged if pos in ['NN','NNP'] ]\n",
    "print(allnoun)\n",
    "\n",
    "# How can we get all the verbs in the sentence?\n",
    "tagged = nltk.pos_tag(word_tokenize(string))\n",
    "allverb = [word for word,pos in tagged if pos in ['VBD','VBG', 'VB', 'VBN'] ]\n",
    "print(allverb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Corpus Readers__\n",
    "<br>\n",
    "The nltk.corpus package defines a collection of corpus reader classes, which can be used to access the contents of a diverse set of corpora. The list of available corpora:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<thead valign=\"top\">\n",
    "<tr><th class=\"head\">ID</th>\n",
    "<th class=\"head\">File</th>\n",
    "<th class=\"head\">Genre</th>\n",
    "<th class=\"head\">Description</th>\n",
    "</tr>\n",
    "</thead>\n",
    "\n",
    "<tbody valign=\"top\">\n",
    "<tr><td>A16</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">ca16</span></tt></td>\n",
    "<td>news</td>\n",
    "<td>Chicago Tribune: <em>Society Reportage</em></td>\n",
    "</tr>\n",
    "<tr><td>B02</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">cb02</span></tt></td>\n",
    "<td>editorial</td>\n",
    "<td>Christian Science Monitor: <em>Editorials</em></td>\n",
    "</tr>\n",
    "<tr><td>C17</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">cc17</span></tt></td>\n",
    "<td>reviews</td>\n",
    "<td>Time Magazine: <em>Reviews</em></td>\n",
    "</tr>\n",
    "<tr><td>D12</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">cd12</span></tt></td>\n",
    "<td>religion</td>\n",
    "<td>Underwood: <em>Probing the Ethics of Realtors</em></td>\n",
    "</tr>\n",
    "<tr><td>E36</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">ce36</span></tt></td>\n",
    "<td>hobbies</td>\n",
    "<td>Norling: <em>Renting a Car in Europe</em></td>\n",
    "</tr>\n",
    "<tr><td>F25</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">cf25</span></tt></td>\n",
    "<td>lore</td>\n",
    "<td>Boroff: <em>Jewish Teenage Culture</em></td>\n",
    "</tr>\n",
    "<tr><td>G22</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">cg22</span></tt></td>\n",
    "<td>belles_lettres</td>\n",
    "<td>Reiner: <em>Coping with Runaway Technology</em></td>\n",
    "</tr>\n",
    "<tr><td>H15</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">ch15</span></tt></td>\n",
    "<td>government</td>\n",
    "<td>US Office of Civil and Defence Mobilization: <em>The Family Fallout Shelter</em></td>\n",
    "</tr>\n",
    "<tr><td>J17</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">cj19</span></tt></td>\n",
    "<td>learned</td>\n",
    "<td>Mosteller: <em>Probability with Statistical Applications</em></td>\n",
    "</tr>\n",
    "<tr><td>K04</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">ck04</span></tt></td>\n",
    "<td>fiction</td>\n",
    "<td>W.E.B. Du Bois: <em>Worlds of Color</em></td>\n",
    "</tr>\n",
    "<tr><td>L13</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">cl13</span></tt></td>\n",
    "<td>mystery</td>\n",
    "<td>Hitchens: <em>Footsteps in the Night</em></td>\n",
    "</tr>\n",
    "<tr><td>M01</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">cm01</span></tt></td>\n",
    "<td>science_fiction</td>\n",
    "<td>Heinlein: <em>Stranger in a Strange Land</em></td>\n",
    "</tr>\n",
    "<tr><td>N14</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">cn15</span></tt></td>\n",
    "<td>adventure</td>\n",
    "<td>Field: <em>Rattlesnake Ridge</em></td>\n",
    "</tr>\n",
    "<tr><td>P12</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">cp12</span></tt></td>\n",
    "<td>romance</td>\n",
    "<td>Callaghan: <em>A Passion in Rome</em></td>\n",
    "</tr>\n",
    "<tr><td>R06</td>\n",
    "<td><tt class=\"doctest\"><span class=\"pre\">cr06</span></tt></td>\n",
    "<td>humor</td>\n",
    "<td>Thurber: <em>The Future, If Any, of Comedy</em></td>\n",
    "</tr>\n",
    "</tbody>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ EXAMPLE of BROWN CORPUS __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BROWN CORPUS\n",
      "\n",
      "A Standard Corpus of Present-Day Edited American\n",
      "English, for use with Digital Computers.\n",
      "\n",
      "by W. N. Francis and H. Kucera (1964)\n",
      "Department of Linguistics, Brown University\n",
      "Providence, Rhode Island, USA\n",
      "\n",
      "Revised 1971, Revised and Amplified 1979\n",
      "\n",
      "http://www.hit.uib.no/icame/brown/bcm.html\n",
      "\n",
      "Distributed with the permission of the copyright holder,\n",
      "redistribution permitted.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ['adventure', 'belles_lettres']\n",
      "\n",
      "\n",
      "\n",
      " ['ca01', 'ca02']\n",
      "\n",
      "\n",
      "\n",
      " ['The', 'Fulton']\n",
      "\n",
      "\n",
      "\n",
      " ['Does', 'our']\n",
      "\n",
      "\n",
      "\n",
      " [[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']]]\n",
      "\n",
      "\n",
      "\n",
      " [[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]]]\n",
      "\n",
      "\n",
      "\n",
      " ['The', 'Fulton']\n",
      "\n",
      "\n",
      "\n",
      " [('The', 'AT'), ('Fulton', 'NP-TL')]\n",
      "\n",
      "\n",
      "\n",
      " [['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']]\n",
      "\n",
      "\n",
      "\n",
      " [[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "print(\"\\n\", brown.readme())\n",
    "print(\"\\n\\n\\n\", brown.categories()[0:2])\n",
    "print(\"\\n\\n\\n\", brown.fileids()[0:2])\n",
    "print(\"\\n\\n\\n\", brown.words(fileids=['cg22'])[0:2] )\n",
    "print(\"\\n\\n\\n\", brown.paras(categories='news')[0:1])\n",
    "print(\"\\n\\n\\n\", brown.tagged_paras(categories='news')[0:1])\n",
    "print(\"\\n\\n\\n\", brown.words(categories='news')[0:2])\n",
    "print(\"\\n\\n\\n\", brown.tagged_words(categories='news')[0:2])\n",
    "print(\"\\n\\n\\n\", brown.sents(categories='news')[0:1])\n",
    "print(\"\\n\\n\\n\", brown.tagged_sents(categories='news')[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default tagging- DefaultTagger__ <br>\n",
    "Default tagging provides a baseline for part-of-speech tagging. It simply assigns the same\n",
    "part-of-speech tag to every token. We do this using the DefaultTagger class. This tagger\n",
    "is useful as a last-resort tagger, and provides a baseline to measure accuracy improvements.\n",
    "\n",
    "The DefaultTagger class takes a single argument, the tag you want to apply. We'll give it\n",
    "NN, which is the tag for a singular noun. DefaultTagger is most useful when you choose\n",
    "the most common part-of-speech tag. Since nouns tend to be the most common types of\n",
    "words, a noun tag is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NN'), ('World', 'NN')]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "\n",
    "tagger = DefaultTagger('NN')\n",
    "tagger.tag(['Hello', 'World'])\n",
    "\n",
    "\"\"\"Every tagger has a tag() method that takes a list of tokens, where each token is a single\n",
    "word. This list of tokens is usually a list of words produced by a word tokenizer\"\"\"\n",
    "\n",
    "\"\"\"tag() returns a list of tagged tokens, where a tagged token is a tuple of (word, tag).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 218 samples and 100554 outcomes>\n",
      "0.13089484257215028\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.tag import DefaultTagger\n",
    "tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
    "print(nltk.FreqDist(tags))\n",
    "\n",
    "\"\"\"We can see NN comes as the most frequent tag, so let's start building a very naïve POS tagger, by\n",
    "assigning NN as a tag to all the test words. NLTK has a DefaultTagger function that can be used for\n",
    "this. DefaultTagger function is part of the Sequence tagger, which will be discussed next. There is a\n",
    "function called evaluate() that gives the accuracy of the correctly predicted POS of the words. This\n",
    "is used to benchmark the tagger against the brown corpus. In the default_tagger case, we are\n",
    "getting approximately 13 percent of the predictions correct. We will use the same benchmark for all the\n",
    "taggers moving forward.\"\"\"\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "default_tagger = DefaultTagger('NN')\n",
    "print(default_tagger.evaluate(brown_tagged_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8361407355726104\n",
      "0.8452108043456593\n",
      "0.843317053722715\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\"\"\"Unigram just considers the conditional frequency of tags and predicts the most frequent tag for the every\n",
    "given token. The bigram_tagger parameter will consider the tags of the given word and the\n",
    "previous word, and tag as tuple to get the given tag for the test word. The TrigramTagger parameter\n",
    "looks for the previous two words with a similar process.\"\"\"\n",
    "\n",
    "\n",
    "train_data = brown_tagged_sents[ : int(len(brown_tagged_sents) * 0.9)]\n",
    "test_data = brown_tagged_sents[int(len(brown_tagged_sents) * 0.9) : ]\n",
    "\n",
    "unigram_tagger = UnigramTagger(train_data,backoff=default_tagger)\n",
    "print(unigram_tagger.evaluate(test_data))\n",
    "\n",
    "bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print(bigram_tagger.evaluate(test_data))\n",
    "\n",
    "trigram_tagger = TrigramTagger(train_data,backoff=bigram_tagger)\n",
    "print(trigram_tagger.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
